# Document Intelligence System - Implementation Plan

## ğŸ¯ Goal
Add business context from client documents (PDFs, Excel, CSV, Word) to enhance AI reply quality for Instagram comments.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Storage    â”‚ â† Store original documents
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docling Service â”‚ â† Extract & convert to Markdown
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL DB  â”‚ â† Store metadata + markdown context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reply Agent    â”‚ â† Use context in responses
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Database Schema Design

### New Table: `client_documents`

```sql
CREATE TABLE client_documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),

    -- Client identification
    client_id VARCHAR(100) NOT NULL,  -- Instagram business account ID
    client_name VARCHAR(200),          -- Business name

    -- Document metadata
    document_name VARCHAR(500) NOT NULL,
    document_type VARCHAR(50) NOT NULL,  -- 'pdf', 'excel', 'csv', 'word', 'txt'
    description TEXT,                    -- Human-readable description

    -- S3 storage
    s3_bucket VARCHAR(200) NOT NULL,
    s3_key VARCHAR(500) NOT NULL,        -- S3 object key
    s3_url TEXT NOT NULL,                -- Full S3 URL (presigned or public)
    file_size_bytes BIGINT,              -- Original file size

    -- Processed content
    markdown_content TEXT,               -- Extracted content in markdown format
    content_hash VARCHAR(64),            -- SHA-256 hash for deduplication

    -- Processing status
    processing_status VARCHAR(50) DEFAULT 'pending',  -- pending, processing, completed, failed
    processing_error TEXT,               -- Error message if failed
    processed_at TIMESTAMP,              -- When processing completed

    -- AI/Search metadata
    embedding_status VARCHAR(50) DEFAULT 'pending',  -- For future vector search
    tags JSONB,                          -- Categorization tags

    -- Audit fields
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    created_by VARCHAR(100),             -- User/system who uploaded

    -- Indexes
    INDEX idx_client_documents_client_id (client_id),
    INDEX idx_client_documents_status (processing_status),
    INDEX idx_client_documents_type (document_type),
    INDEX idx_client_documents_hash (content_hash),

    -- Constraints
    UNIQUE (client_id, content_hash),    -- Prevent duplicate documents
    CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed')),
    CHECK (document_type IN ('pdf', 'excel', 'csv', 'word', 'txt', 'other'))
);
```

## ğŸ”§ Technology Stack

### 1. **Docling** - Document Processing
- Best choice for PDF extraction
- Supports multiple formats
- Outputs clean markdown
- Handles tables, images, structure

```python
from docling.document_converter import DocumentConverter

converter = DocumentConverter()
result = converter.convert("document.pdf")
markdown = result.document.export_to_markdown()
```

### 2. **AWS S3 (boto3)** - Document Storage
```python
import boto3
s3_client = boto3.client('s3')
```

### 3. **Celery** - Async Processing
```python
@celery_app.task
def process_document_task(document_id: str):
    # Download from S3 â†’ Process with Docling â†’ Save markdown
    pass
```

## ğŸ“ File Structure

```
src/core/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ client_document.py          # SQLAlchemy model
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ s3_service.py                # S3 upload/download
â”‚   â”œâ”€â”€ document_processing_service.py  # Docling integration
â”‚   â””â”€â”€ document_context_service.py  # Context retrieval for agent
â”œâ”€â”€ tasks/
â”‚   â””â”€â”€ document_tasks.py            # Celery tasks
â””â”€â”€ schemas/
    â””â”€â”€ document_schemas.py          # Pydantic models

src/api_v1/
â””â”€â”€ documents/
    â”œâ”€â”€ views.py                     # REST endpoints
    â””â”€â”€ schemas.py                   # Request/response models
```

## ğŸ”„ Processing Flow

### 1. Upload Flow
```
User uploads document
    â†“
API receives file
    â†“
Upload to S3
    â†“
Create DB record (status: pending)
    â†“
Queue Celery task
    â†“
Return document_id to user
```

### 2. Processing Flow (Celery Task)
```
Download from S3
    â†“
Detect document type
    â†“
Process with Docling
    â†“
Convert to Markdown
    â†“
Calculate content hash
    â†“
Update DB record (markdown_content, status: completed)
    â†“
Optional: Generate embeddings for vector search
```

### 3. Query Flow (Reply Agent)
```
Receive Instagram comment
    â†“
Identify client_id from media owner
    â†“
Fetch client documents (markdown_content)
    â†“
Include in agent context
    â†“
Generate informed response
```

## ğŸ¨ Implementation Steps

### Phase 1: Core Infrastructure âœ…
1. âœ… Create database model
2. âœ… Create migration
3. âœ… Add S3 service
4. âœ… Add Docling processing service

### Phase 2: API & Processing âœ…
5. âœ… Create API endpoints (upload, list, get, delete)
6. âœ… Implement Celery task for processing
7. âœ… Add document context service

### Phase 3: Agent Integration âœ…
8. âœ… Modify reply agent to fetch document context
9. âœ… Add context to agent prompts
10. âœ… Test end-to-end flow

### Phase 4: Testing & Polish âœ…
11. âœ… Unit tests
12. âœ… Integration tests
13. âœ… Documentation

## ğŸ“ Example Usage

### Upload Document
```bash
curl -X POST http://localhost:4291/api/v1/documents/upload \
  -F "file=@business_catalog.pdf" \
  -F "client_id=instagram_account_123" \
  -F "client_name=Real Estate Agency" \
  -F "description=Product catalog with prices"
```

### Response
```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "document_name": "business_catalog.pdf",
  "processing_status": "pending",
  "s3_url": "https://s3.amazonaws.com/bucket/docs/...",
  "created_at": "2025-10-03T20:00:00Z"
}
```

### Query Documents
```bash
curl http://localhost:4291/api/v1/documents?client_id=instagram_account_123
```

### Get Document
```bash
curl http://localhost:4291/api/v1/documents/550e8400-e29b-41d4-a716-446655440000
```

## ğŸ”Œ Agent Integration

### Before (no context):
```python
agent_input = f"Comment: {comment_text}\nMedia: {media_caption}"
```

### After (with context):
```python
# Fetch client documents
docs = await document_context_service.get_client_context(client_id)

agent_input = f"""
Client Business Context:
{docs}

---
Media: {media_caption}
Comment from @{username}: {comment_text}

Please provide a helpful, context-aware response.
"""
```

## ğŸ“¦ Dependencies to Add

```toml
[tool.poetry.dependencies]
# Document processing
docling = "^2.0.0"             # PDF/document to markdown
python-magic = "^0.4.27"       # File type detection

# AWS S3
boto3 = "^1.34.0"              # AWS SDK
botocore = "^1.34.0"

# Optional: Vector search (future)
# pgvector = "^0.3.0"          # PostgreSQL vector extension
```

## ğŸ¯ Benefits

âœ… **Better responses** - AI has business context
âœ… **Scalable** - S3 + async processing
âœ… **Production-ready** - Error handling, retries
âœ… **Flexible** - Supports multiple document types
âœ… **Efficient** - Markdown format optimized for LLMs
âœ… **Deduplication** - Content hash prevents duplicates
âœ… **Searchable** - Can add vector search later

## ğŸš€ Future Enhancements

1. **Vector Search** - Use pgvector for semantic document search
2. **Chunking** - Split large documents for better retrieval
3. **Multi-language** - OCR for scanned documents
4. **Excel Processing** - Extract structured data from spreadsheets
5. **Auto-tagging** - AI-powered document categorization
6. **Webhooks** - Notify when processing completes
7. **Versioning** - Track document updates

## ğŸ”’ Security Considerations

- âœ… Presigned S3 URLs (expiring links)
- âœ… Client ID isolation (can't access other clients' docs)
- âœ… File type validation
- âœ… Size limits (prevent abuse)
- âœ… Virus scanning (optional with ClamAV)
- âœ… Access logs

## ğŸ“Š Monitoring

- Processing success/failure rates
- Average processing time
- S3 storage usage
- Document count per client
- Agent context usage

## Ready to implement?

Let me know if you want me to start implementing this system! I'll create:
1. Database model
2. Migration
3. S3 service
4. Docling processing service
5. API endpoints
6. Celery tasks
7. Agent integration
8. Tests

This will be a production-ready document intelligence system! ğŸš€
